{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41bf932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd12315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how it works \n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cce847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "4 0.0 False\n"
     ]
    }
   ],
   "source": [
    "#we can render in different mode.\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\", is_slippery = True) \n",
    "env = env.unwrapped         #because it wrapped in timecounter to give truncated error\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(env.render())\n",
    "\n",
    "action = env.action_space.sample()\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "print(env.render())\n",
    "\n",
    "print(state, reward, terminated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7274f9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 5, 0.0, True),\n",
       " (0.3333333333333333, 2, 0.0, False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this env will give you the transition matrix\n",
    "env.P[1][1]\n",
    "# state, action, transion prob, next state, reward, Termination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782d1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** after epoch  1 ***\n",
      "the state value \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "the best policy \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "*** after epoch  2 ***\n",
      "the state value \n",
      " [[0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.   0.   0.3  0.  ]\n",
      " [0.   0.3  1.39 0.  ]]\n",
      "the best policy \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 1. 0.]]\n",
      "*** after epoch  3 ***\n",
      "the state value \n",
      " [[0.     0.     0.     0.    ]\n",
      " [0.     0.     0.09   0.    ]\n",
      " [0.     0.18   0.498  0.    ]\n",
      " [0.     0.561  1.5853 0.    ]]\n",
      "the best policy \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  4 ***\n",
      "the state value \n",
      " [[0.       0.       0.027    0.0081  ]\n",
      " [0.       0.       0.1575   0.      ]\n",
      " [0.054    0.3339   0.62301  0.      ]\n",
      " [0.       0.74406  1.698808 0.      ]]\n",
      "the best policy \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  5 ***\n",
      "the state value \n",
      " [[0.         0.0081     0.05778    0.022194  ]\n",
      " [0.0162     0.         0.204237   0.        ]\n",
      " [0.12123    0.44649    0.7048605  0.        ]\n",
      " [0.         0.8668074  1.76968462 0.        ]]\n",
      "the best policy \n",
      " [[0. 1. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  6 ***\n",
      "the state value \n",
      " [[0.00729    0.021951   0.0852633  0.03889539]\n",
      " [0.043416   0.         0.23703714 0.        ]\n",
      " [0.1833408  0.52650261 0.75996731 0.        ]\n",
      " [0.         0.94889839 1.8155749  0.        ]]\n",
      "the best policy \n",
      " [[1. 3. 2. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  7 ***\n",
      "the state value \n",
      " [[0.0217971  0.03870342 0.10835875 0.05584486]\n",
      " [0.07456617 0.         0.26049782 0.        ]\n",
      " [0.23532287 0.58325657 0.79779879 0.        ]\n",
      " [0.         1.00431896 1.84596816 0.        ]]\n",
      "the best policy \n",
      " [[2. 3. 2. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  8 ***\n",
      "the state value \n",
      " [[0.04052001 0.05627465 0.12753937 0.07176873]\n",
      " [0.10512272 0.         0.27760145 0.        ]\n",
      " [0.27711065 0.62376852 0.82420144 0.        ]\n",
      " [0.         1.04221669 1.86645545 0.        ]]\n",
      "the best policy \n",
      " [[1. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  9 ***\n",
      "the state value \n",
      " [[0.06057521 0.07331677 0.14353727 0.08612242]\n",
      " [0.13284257 0.         0.29032161 0.        ]\n",
      " [0.31011652 0.65296039 0.84292124 0.        ]\n",
      " [0.         1.06848976 1.88048357 0.        ]]\n",
      "the best policy \n",
      " [[2. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  10 ***\n",
      "the state value \n",
      " [[0.08002037 0.08906232 0.15687636 0.09873636]\n",
      " [0.15689384 0.         0.29993928 0.        ]\n",
      " [0.33599123 0.67422067 0.85639305 0.        ]\n",
      " [0.         1.0869582  1.89023253 0.        ]]\n",
      "the best policy \n",
      " [[1. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  11 ***\n",
      "the state value \n",
      " [[0.09779296 0.10311949 0.16798054 0.10963598]\n",
      " [0.17720341 0.         0.30731208 0.        ]\n",
      " [0.35622459 0.68987275 0.86622521 0.        ]\n",
      " [0.         1.10011904 1.89710547 0.        ]]\n",
      "the best policy \n",
      " [[1. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  12 ***\n",
      "the state value \n",
      " [[0.11343476 0.11536044 0.17719592 0.11894036]\n",
      " [0.19405883 0.         0.31302634 0.        ]\n",
      " [0.37204685 0.70151733 0.87349474 0.        ]\n",
      " [0.         1.10962255 1.90201841 0.        ]]\n",
      "the best policy \n",
      " [[1. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  13 ***\n",
      "the state value \n",
      " [[0.12685621 0.12582377 0.18481381 0.12680836]\n",
      " [0.20788857 0.         0.31749256 0.        ]\n",
      " [0.38443582 0.71026594 0.87893307 0.        ]\n",
      " [0.         1.11657207 1.90557714 0.        ]]\n",
      "the best policy \n",
      " [[1. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  14 ***\n",
      "the state value \n",
      " [[0.13848029 0.13473536 0.19111252 0.13341877]\n",
      " [0.2192414  0.         0.32101368 0.        ]\n",
      " [0.39418295 0.71690643 0.88304917 0.        ]\n",
      " [0.         1.12171669 1.90818815 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  15 ***\n",
      "the state value \n",
      " [[0.1488606  0.14241254 0.19636162 0.13895975]\n",
      " [0.22868549 0.         0.32382324 0.        ]\n",
      " [0.40193246 0.7220095  0.88620627 0.        ]\n",
      " [0.         1.1255743  1.91012874 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  16 ***\n",
      "the state value \n",
      " [[0.157922   0.14900885 0.20075811 0.14360328]\n",
      " [0.23656198 0.         0.32608931 0.        ]\n",
      " [0.40815118 0.72597953 0.88865927 0.        ]\n",
      " [0.         1.12850477 1.91159005 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  17 ***\n",
      "the state value \n",
      " [[0.1657218  0.15464663 0.20444822 0.14749644]\n",
      " [0.24313049 0.         0.32793225 0.        ]\n",
      " [0.41317836 0.72910272 0.89058751 0.        ]\n",
      " [0.         1.13075926 1.91270479 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  18 ***\n",
      "the state value \n",
      " [[0.17237223 0.15944012 0.20754618 0.15076171]\n",
      " [0.24860432 0.         0.3294401  0.        ]\n",
      " [0.41726562 0.73158372 0.89211858 0.        ]\n",
      " [0.         1.13251433 1.91356574 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  19 ***\n",
      "the state value \n",
      " [[0.17800463 0.16349728 0.21014507 0.15350055]\n",
      " [0.25316237 0.         0.3306791  0.        ]\n",
      " [0.42060351 0.73357093 0.89334473 0.        ]\n",
      " [0.         1.1338953  1.91423831 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "*** after epoch  20 ***\n",
      "the state value \n",
      " [[0.18275149 0.16691815 0.21232269 0.15579714]\n",
      " [0.25695521 0.         0.33170023 0.        ]\n",
      " [0.4233389  0.73517368 0.89433366 0.        ]\n",
      " [0.         1.13499219 1.91476915 0.        ]]\n",
      "the best policy \n",
      " [[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Value Iteration\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "gamma = 0.9\n",
    "epoch = 20\n",
    "\n",
    "state_value = np.zeros(num_states)\n",
    "policy = np.zeros(num_states)\n",
    "for i in range(epoch):\n",
    "    for state in range(num_states):\n",
    "        action_value = np.zeros(num_actions)\n",
    "        for action in range(num_actions):\n",
    "            # P[1][2] will give info about possible next state\n",
    "            for TransProb, nextstate, reward, terminated in env.P[state][action]:\n",
    "                action_value[action] += reward + gamma * TransProb * state_value[nextstate]\n",
    "\n",
    "        state_value[state] = max(action_value)\n",
    "        policy[state] = np.argmax(action_value)\n",
    "    print(\"*** after epoch \", i+1,\"***\")\n",
    "    print(\"the state value \\n\", state_value.reshape(4,4))\n",
    "    print(\"the best policy \\n\", policy.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffda0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this works very well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
